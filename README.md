# NjuSearch
**项目概述：**

通过网络爬虫获得学校各个网站的链接，再通过python进一步爬取网页的文本信息（涉及安全，这部分代码未给出），使用BeautifulSoup对文本进行解析处理存入数据库中。用户输入其关心的问题，前台使用php调用分词模块进行分词，再与后台的python脚本通信，通过一定的算法查询。并以较为简洁方式将最后结果呈现给用户。

**项目搭建：**

 - 服务器配置:
 Centos系统
 
 - 数据库：
 Mongodb数据库
 - 网页后台:
 使用jieba中文分词，将文本进行处理，并以json格式存入数据库
 
 - 网页前台：
通过html和css进行了前台的设计并用JS实现了部分功能，用php调用SCWS分词

 - 核心算法：
 BM25算法

**项目总结：**

 1. BM25算法运用：动态进行数据的读取和运算较为耗时，所以对网页内容的打分可以先进行计算，并打分后存入数据库。
 2. 分词：jieba分词分词效果不错，但是在前台调用用python写的分词脚本对用户输入问题进行分词较为耗时，大约要3秒左右，故改为用php直接调用SCWS分词，分词较为迅速。
 3. 数据库：mongodb较为消耗内存，当用户数目上升较快时，内存消耗增长快。
 
